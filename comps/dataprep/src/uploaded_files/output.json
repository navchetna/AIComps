{
    "root": {
        "content": [
            ""
        ],
        "children": [
            {
                "2 Related Work": {
                    "content": [
                        "Task-specific Distillation Sun et al. (2019b) taskspecifically compressed BERT by learning from the every k-th layer of the teacher. To avoid leaving out some of the teacher layers, many follow-up works (Wu et al., 2020, Passban et al., 2021, Wu et al., 2021) designed new layer mapping strategies to fuse the teacher layers. Jiao et al. (2020) used data augmentation to further improve the performance. Initialising the student model with pretrained weights is crucial for performance since the student learns from the teacher only shortly in downstream tasks. Common choices for initialization are: (1) task-agnostically distilling models first, (2) using publicly available distilled models, or (3) initializing with teacher layers. As part of this study, we examine how to maximize the benefits of initializing from teacher layers.\n**Task-agnostic Distillation** In the field of task-agnostic distillation, one line of work is to compress the teacher model into a student model with the same depth but narrower blocks (Sun et al., 2020b, Zhang et al., 2022). Another line of work is to distill the teacher into a student with fewer layers (Sanh et al., 2019, Jiao et al., 2020, Wang et al., 2020, Wang et al., 2020, Wang et al., 2021), which is our focus.\n**Comparative Studies** Li et al. (2021) conducted out-of-domain and adversarial evaluation on three KD methods, which used hidden state transfer or data augmentation. Lu et al. (2022) is closely related to our work, where they also evaluated knowledge types and initialisation schemes. However, they did not consider layer choice when initialising from the teacher, and the evaluation was only for task-specific settings. Hence, our work complements theirs.\n"
                    ],
                    "children": []
                }
            },
            {
                "3 Distillation Objectives": {
                    "content": [
                        "**Prediction Layer Transfer** Prediction layer transfer minimizes the soft cross-entropy between the logits from the teacher and the student:  $\\mathcal{L}_{\\text{pred}} = \\text{CE}(\\boldsymbol{z}^T/t, \\boldsymbol{z}^S/t)$ , with  $\\boldsymbol{z}^T$  and  $\\boldsymbol{z}^S$  the logits from the teacher/student and t is the temperature value.\nFollowing the vanilla KD approach (Hinton et al., 2015), the final training loss is a combination of  $\\mathcal{L}_{pred}$  and supervision loss  $\\mathcal{L}_{ce}$  (masked language modelling loss  $\\mathcal{L}_{mlm}$  in the pertaining stage). We denote this objective as **vanilla KD**.\nHidden States Transfer Hidden states transfer penalizes the distance between the hidden states of specific layers from the teacher and the student. Common choices for the representation are the embedding of the [CLS] token (Sun et al., 2019b) and the whole sequence embedding (Jiao et al., 2020). We use Mean-Squared-Error (MSE) to measure the distance between the student and teacher embedding, which can be formulated as  $\\mathcal{L}_{\\text{hid}} = \\text{MSE}(\\boldsymbol{h}^{S}\\boldsymbol{W}_{h}, \\boldsymbol{h}^{T})$ , where  $\\boldsymbol{h}^{S} \\in \\mathbb{R}^{d}$  and  $oldsymbol{h}^T \\in \\mathbb{R}^{d'}$  are the <code>[CLS]</code> token embedding of specific student and teacher layer, d and d' are the hidden dimensions. The matrix  $\\boldsymbol{W}_h \\in \\mathbb{R}^{d \\times d'}$  is a learnable transformation. We denote this objective as Hid-CLS. In the case of transferring the sequence embedding, one can replace the token embeddings with sequence embeddings  $\\boldsymbol{H}^{S} \\in \\mathbb{R}^{l imes d}$ and  $H^T \\in \\mathbb{R}^{l \\times d'}$ , where *l* is the sequence length. The objective that transfers the sequence embedding with MSE loss is denoted as Hid-Seq.\nWe also evaluated a contrastive representation learning method which transfers the hidden state representation from the teacher to the student with a contrastive objective (Sun et al., 2020a). We inherited their code for implementation and refer our readers to the original paper for details. We denote this objective as **Hid-CLS-Contrast**.\nAttention and Value Transfer The attention mechanism has been found to capture rich linguistic knowledge (Clark et al., 2019), and attention map transfer is widely used in transformer model distillation. To measure the similarity between the multi-head attention block of the teacher and the student, MSE and Kullback-Leibler divergence are the two standard loss functions. The objective using MSE is formulated as  $\\mathcal{L}_{\\text{att}} = \\frac{1}{h} \\sum_{i=1}^{h} \\text{MSE}(\\mathbf{A}_{i}^{S}, \\mathbf{A}_{i}^{T})$ , where *h* is the number of attention heads, matrices  $\\mathbf{A}_{i} \\in \\mathbb{R}^{l \\times l}$  refers to the *i*-th attention head (before the softmax operation) in the multi-head attention block. We denote this objective as Att-MSE.\nSince the attention after the softmax function is a distribution over the sequence, we can also use the KL-divergence to measure the distance:  $\\mathcal{L}_{att} = \\frac{1}{TH} \\sum_{t=1}^{T} \\sum_{h=1}^{H} D_{KL}(a_{t,h}^{T} || a_{t,h}^{S})$ , where T is the sequence length and H is the number of attention heads. We will denote this objective as **Att-KL**. In addition to attention transfer, value-relation transfer was proposed by Wang et al. (2020), to which we refer our readers for details. Value-relation transfer objective will be denoted as **Val-KL**.\n",
                        "| Objectives       | QNLI<br>Acc | SST-2<br>Acc | MNLI<br>Acc | MRPC<br>F1 | QQP<br>Acc | RTE<br>Acc | CoLA<br>Mcc | Avg  |\n|------------------|-------------|--------------|-------------|------------|------------|------------|-------------|------|\n| Vanilla KD       | 66.5\u00b11.49   | 84.7\u00b10.16    | 75.1\u00b10.05   | 71.2\u00b10.80  | 81.9\u00b10.10  | 54.0\u00b11.24  | 69.1\u00b10.00   | 71.8 |\n| Hid-CLS-Contrast | 69.3\u00b10.60   | 85.3\u00b10.56    | 76.2\u00b10.45   | 71.1\u00b10.85  | 83.1\u00b10.69  | 53.6\u00b10.23  | 69.0\u00b10.12   | 72.5 |\n| Hid-CLS          | 75.7\u00b10.57   | 85.8\u00b10.34    | 77.0\u00b10.10   | 71.3\u00b10.41  | 83.8\u00b11.63  | 54.0\u00b12.17  | 68.4\u00b10.35   | 73.2 |\n| Hid-Seq          | 83.3\u00b10.13   | 87.4\u00b10.13    | 78.3\u00b10.13   | 72.9\u00b10.50  | 87.6\u00b10.00  | 51.8\u00b11.10  | 69.2\u00b10.55   | 75.8 |\n| Att-MSE          | 84.3\u00b10.18   | 89.2\u00b10.40    | 78.6\u00b10.25   | 71.1\u00b10.41  | 88.7\u00b10.05  | 54.4\u00b11.03  | 69.3\u00b10.17   | 76.5 |\n| +Hid-Seq         | 84.6\u00b10.29   | 89.2\u00b10.21    | 78.9\u00b10.10   | 71.8\u00b10.51  | 88.8\u00b10.00  | 54.0\u00b10.93  | 69.5\u00b10.48   | 77.0 |\n| Att-KL           | 85.3\u00b10.14   | 89.0\u00b10.26    | 79.4\u00b10.08   | 71.4\u00b10.29  | 89.0\u00b10.05  | 55.5\u00b12.05  | 69.3\u00b10.13   | 77.0 |\n| +Hid-Seq         | 84.6\u00b10.21   | 89.1\u00b10.46    | 79.5\u00b10.17   | 72.4\u00b10.39  | 89.0\u00b10.06  | 57.2\u00b10.86  | 69.3\u00b10.21   | 77.3 |\n| +Val-KL          | 85.5\u00b10.24   | 89.6\u00b10.31    | 79.6\u00b10.10   | 72.2\u00b10.39  | 89.1\u00b10.05  | 57.5\u00b10.70  | 69.2\u00b10.15   | 77.5 |\n",
                        "| Objectives  | QNLI<br>Acc | SST-2<br>Acc | MNLI<br>Acc | MRPC<br>F1 | QQP<br>Acc | RTE<br>Acc | CoLA<br>Mcc | Avg  |\n|-------------|-------------|--------------|-------------|------------|------------|------------|-------------|------|\n| DistilBERT\u22c6 | 89.2        | 91.3         | 82.2        | 87.5       | 88.5       | 59.9       | 51.3        | 78.5 |\n| TinyBERT\u2020   | 90.5        | 91.6         | 83.5        | 88.4       | 90.6       | 72.2       | 42.8        | 79.9 |\n| MiniLM\u00a7     | 91.0        | 92.0         | 84.0        | 88.4       | 91.0       | 71.5       | 49.2        | 81.0 |\n| Vanilla KD\u22c6 | 88.6        | 91.4         | 82.4        | 86.5       | 90.6       | 61.0       | 54.4        | 79.3 |\n| Hid-CLS     | 86.5        | 90.6         | 79.3        | 73.0       | 89.7       | 61.0       | 33.9        | 73.4 |\n| Hid-Seq     | 89.2        | 91.5         | 82.3        | 89.2       | 90.3       | 67.2       | 48.2        | 79.7 |\n| Att-MSE     | 89.8        | 91.6         | 83.2        | 90.6       | 90.7       | 69.7       | 53.5        | 81.3 |\n| +Hid-Seq\u2020   | 89.7        | 92.4         | 82.8        | 90.4       | 90.8       | 68.6       | 52.8        | 81.1 |\n| Att-KL      | 88.0        | 89.7         | 81.1        | 90.1       | 90.3       | 66.1       | 43.6        | 78.4 |\n| +Hid-Seq    | 88.9        | 91.6         | 82.4        | 90.0       | 90.5       | 66.8       | 47.9        | 79.7 |\n| +Val-KL\u00a7    | 89.8        | 91.6         | 82.4        | 91.0       | 90.6       | 66.7       | 47.7        | 80.0 |\n"
                    ],
                    "children": []
                }
            },
            {
                "4 Experimental Setup": {
                    "content": [
                        "We evaluate our model on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) tasks, including linguistic acceptability (CoLA), sentiment analysis (SST-2), semantic equivalence (MRPC, QQP), and natural language inference (MNLI, QNLI, RTE).\nFor task-specific distillation, we distill a finetuned RoBERTaBASE (Liu et al., 2019) into a 3 layer transformer model on each GLUE task, using the Fairseq (Ott et al., 2019) implementation and the recommended hyperparameters presented in Liu et al. (2019). We follow the training procedure from TinyBERT to perform *intermediate layer* and *prediction layer* distillation sequentially for 10 epochs each, freeing us from tuning the loss weights. For intermediate layer distillation, the\nstudent learns from the same teacher's layers that were used for initialising the student. In addition, we always initialise the embedding layer with the teacher's embedding layer.\nFor task-agnostic distillation, we distill the uncased version of BERTbase into a 6-layer student model, based on the implementation by Izsak et al. (2021). Here we perform last-layer knowledge transfer since we see no improvement when transferring multiple layers in our experiments. We train the student model for 100k steps with batch size 1024, a peaking learning rate of 5e-4 and a maximum sequence length of 128. The distilled student model is then fine-tuned on the GLUE datasets with grid search over batch size {16, 32} and learning rate {1e-5, 3e-5, 5e-5, 8e-5}. We follow the original training corpus of BERT: English Wikipedia and BookCorpus (Zhu et al., 2015)."
                    ],
                    "children": []
                }
            }
        ]
    }
}